{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TODO</h3> \n",
    "Fill in any place that says <code># YOUR CODE HERE</code> (make sure to remove the line <code>raise NotImplementedError()</code>).\n",
    "\n",
    "<h3>Suggestions</h3>\n",
    "\n",
    "- To speed up your code, think about how certain operations can be done at the same time.\n",
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "- Double check your code does not have $\\infty$-loops, these will crash the autograder.\n",
    "\n",
    "<h3>Rules</h3>\n",
    "\n",
    "- Blank cells in the notebook are hidden tests. **Do not delete or alter these cells as this will cause the tests to fail automatically**.\n",
    "- **Do not copy any cell**. Instead create a new cell and transfer any code into the new cell.\n",
    "- Do not create multiple python notebooks (.ipynb files).\n",
    "- Do not import any new python packages (this may cause hidden tests to fail).\n",
    "- Each cell must run for less than 5 minutes (there exists a solution with full marks).\n",
    "- **Do not plagiarise!** We take violations of this very seriously. In previous years we have identified instances of plagiarism and reported them to the Senior Teaching & Learning Administrator.\n",
    "- If you are happy with your current grade you do not need to resubmit, the most recent grade from the autograder will be your final grade.\n",
    "- **Only the final submission will be marked!** We will not retain marks or submissions for intermediate runs, so make sure you are happy with your final submission. Start early to take advantage of the multiple autograder runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b50351bd7dbcf201e978920a53b2f029",
     "grade": false,
     "grade_id": "cell-4beed8a45acd7af1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import time\n",
    "from l2distance import l2distance\n",
    "import visclassifier\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "\n",
    "import pylab\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "#new torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#misc imports\n",
    "import random\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea40f1579f4d5e51cd23d84923142e2e",
     "grade": false,
     "grade_id": "cell-5f452d4d6d57362d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, you will implement a linear support vector machine and one operating in kernel space. For this you will need to formulate the primal and dual optimization problems as quadratic programs. For this, we will be dipping into the shallow end with Course staffs' favorite ML framework: PyTorch!\n",
    "\n",
    "For full documentation and details, here is their site https://pytorch.org/. PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, initially developed by Facebook's AI Research lab but now run by its own independent foundation. Pytorch is very neat because, as you have seen your assignments, in order to do gradient descent we've had to calculate gradient manually. No more! Pytorch performs automatic differentation, as long as we use their functions in our code.\n",
    "\n",
    "Note: Because we are working with Pytorch functions and Modules, we will be using excusively Pytorch tensors instead of numpy arrays. This allows us to multiply/use torch parameter objects with our data directly. Pytorch tensors carry most of the same functionality as numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a34c086ce4976a7da04e7a78719ef95",
     "grade": false,
     "grade_id": "cell-63c47f087db65e79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will start with a simple example of PyTorch, where we use gradient descent to find the parameters of a simple linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "140808b7b946f8b1e6423b7176a888bd",
     "grade": false,
     "grade_id": "cell-2b19f0dd3cb286da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gen_regression_data(num_samples = 10000, ndims=1):\n",
    "    # generate random x samples for training and test sets\n",
    "    xTr = torch.rand(num_samples, ndims)\n",
    "    xTe = torch.rand(int(num_samples * 0.1), ndims)\n",
    "    \n",
    "    # construct random w and b vectors\n",
    "    gt_w = torch.randn(ndims, 1)\n",
    "    gt_b = torch.randn(1)\n",
    "    \n",
    "    # gaussian noise for linear regression\n",
    "    noise = np.random.normal(size=(num_samples, 1)) * 0.02\n",
    "    test_noise = np.random.normal(size=(int(num_samples * 0.1), 1)) * 0.02\n",
    "    \n",
    "    # add noise on the labels for the training set\n",
    "    yTr = xTr @ gt_w + gt_b + noise\n",
    "    yTe = xTe @ gt_w + gt_b + test_noise\n",
    "    \n",
    "    return xTr, xTe, yTr, yTe, gt_w, gt_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "767d76436ce46c796db32b28b953a150",
     "grade": false,
     "grade_id": "cell-b7a42c529326718f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lr_xTr, lr_xTe, lr_yTr, lr_yTe, gt_w, gt_b = gen_regression_data(num_samples = 1000, ndims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_yTe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26f38015a831aae01892c4b1750f0649",
     "grade": false,
     "grade_id": "cell-deee2359d73bbc08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we will create our PyTorch model. PyTorch models inherit the class torch.nn.module, and you need to implement the function forward which is equivalent to a forward pass. Usually, you feed in batch of x samples as input and you get batch of outputs, but you could pass other parameters as well if needed. Every torch module will implement two functions. __init__ its constructor, and __forward__ which defines what happens when you call the module.\n",
    "\n",
    "Note we define two fields of the <code>LinearRegressionModel</code>:\n",
    "* <code>self.w</code>: The weight vector of the linear regression model. This is updated automatically by Pytorch in our training loop because we define it as a nn.Parameter (note requires_grad=True). Additionally <code>torch.randn(ndims, 1)</code> gives us an initialization for this weight vector.\n",
    "* <code>self.b</code>: The bias of the linear regression model. This is also updated automatically by Pytorch in our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2590b6f31fbb59256891bba5f3307d3",
     "grade": false,
     "grade_id": "cell-815d937cc7591c3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, ndims):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        \"\"\" pytorch optimizer checks for the properties of the model, and if\n",
    "            the torch.nn.Parameter requires gradient, then the model will update\n",
    "            the parameters automatically.\n",
    "        \"\"\"\n",
    "        self.w = nn.Parameter(torch.randn(ndims, 1), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "419700018fdb6bddc7415cd48cd6ef87",
     "grade": false,
     "grade_id": "cell-a730c75f377a1928",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For this example we use our familiar mean-squared error loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5222540b9ec1c52279f3f399c680768d",
     "grade": false,
     "grade_id": "cell-b0306506b59f6a38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    square_diff = torch.square((y_pred-y_true))\n",
    "    mean_error = 0.5 * torch.mean(square_diff)\n",
    "    return mean_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fd88f655c4a89d8ac33424cd8a2c0f8",
     "grade": false,
     "grade_id": "cell-208d6a365e599862",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here is a generic training loop in Pytorch, **using stochastic gradient descent (SGD) as an optimizer**. We have supplied comments per line to help walk you through what each different part does.\n",
    "\n",
    "If you have used pytorch before, for example with deep learning models, this code will look familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40f8948581e9c84d64d2e82db2bd8a0c",
     "grade": false,
     "grade_id": "cell-76795eebec829d46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_regression_model_sgd(xTr, yTr, num_epochs, reg_param, lr=1e-2, print_freq=100):\n",
    "    ndims = xTr.shape[1]\n",
    "    \n",
    "    model = LinearRegressionModel(ndims)  # initialize the model\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)  # create an SGD optimizer for the model parameters\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # need to zero the gradients in the optimizer so we don't\n",
    "        # use the gradients from previous iterations\n",
    "        optimizer.zero_grad()  \n",
    "        pred = model.forward(xTr)  # compute model predictions\n",
    "        loss = mse_loss(pred, yTr) + reg_param * torch.norm(model.w)\n",
    "        loss.backward()  # compute the gradient wrt loss\n",
    "        optimizer.step()  # performs a step of gradient descent\n",
    "        if (epoch + 1) % print_freq == 0:\n",
    "            print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "    \n",
    "    return model  # return trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_regression_model_sgd(lr_xTr, lr_yTr, num_epochs=1000, reg_param=0.001, lr=1e-2)\n",
    "avg_test_error = mse_loss(model.forward(lr_xTe), lr_yTe)\n",
    "print('avg test error', avg_test_error.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6148658397e6f3eafb8b57e9e0e396c3",
     "grade": false,
     "grade_id": "cell-1224c53c0267d443",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we have a trained model object that we can predict with by passing in input data via model.forward(x). Let's visualize how good of a fit our line is to the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(lr_xTr, model.forward(lr_xTr).detach(),linewidth=5.0, color=\"red\", label=\"Prediction Line\")\n",
    "plt.scatter(lr_xTr, lr_yTr, label=\"Train Points\")\n",
    "plt.scatter(lr_xTe, lr_yTe, label=\"Test Points\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7294ebf29e9bec4dbbb49ef1a79e026c",
     "grade": false,
     "grade_id": "cell-98938cd92d0974bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Note that this might not have converged!** Try re-running the cell above multiple times to see what different results you might get. Adjusting the number of epochs, the regularisation strength, and the learning rate will all affect the solution.\n",
    "\n",
    "Instead, for this assignment, we will use an optimiser that might be unfamiliar to you in pytorch: (L-BFGS)[https://en.wikipedia.org/wiki/Limited-memory_BFGS], an approximate second-order algorithm. While second-order optimisers are not generally preferred for deep learning models, for models which are actually convex or for which mini-batching is unnecessary, this can be far more efficient.\n",
    "\n",
    "The pytorch interface for running the optimiser with L-BFGS is a bit different, and requires defining a `closure` function that executes on each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcdb3845032b96d4f3660bd28373e9e1",
     "grade": false,
     "grade_id": "cell-2b7f5e14b7790b29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_regression_model_lbfgs(xTr, yTr, num_epochs, reg_param):\n",
    "    ndims = xTr.shape[1]\n",
    "    \n",
    "    model = LinearRegressionModel(ndims)  # initialize the model\n",
    "    # * for the optimizer, we use the \"default\" learning rate of 1.0\n",
    "    # * the `line_search_fn` helps determine a step size, but is inappropriate \n",
    "    #   for stochastic objectives (we are doing batch updates here)\n",
    "    optimizer = optim.LBFGS(model.parameters(), lr=1.0, line_search_fn='strong_wolfe') # create an optimizer\n",
    "\n",
    "    def closure():\n",
    "        \"\"\" Function called on every optimiser step \"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        pred = model.forward(xTr)  # compute model predictions\n",
    "        loss = mse_loss(pred, yTr) + reg_param * torch.norm(model.w) # compute loss\n",
    "        loss.backward()  # compute the gradient wrt loss\n",
    "        return loss # return loss\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss = optimizer.step(closure)    \n",
    "        print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "    \n",
    "    return model  # return trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we train the model with L-BFGS and plot the result. Note that although we run it for 10 \"epochs\" (optimiser iterations, or `step`s), the model had actually converged in just one! Each optimiser `.step` will actually call the closure multiple times.\n",
    "\n",
    "We don't use a learning rate parameter here, but you might want to in other applications (e.g. where the objective is not actually convex, or where minibatches are used in each step). We do include a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_regression_model_lbfgs(lr_xTr, lr_yTr, num_epochs=10, reg_param=0.001)\n",
    "avg_test_error = mse_loss(model.forward(lr_xTe), lr_yTe)\n",
    "print('avg test error', avg_test_error.item())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lr_xTr, model.forward(lr_xTr).detach(),linewidth=5.0, color=\"red\", label=\"Prediction Line\")\n",
    "plt.scatter(lr_xTr, lr_yTr, label=\"Train Points\")\n",
    "plt.scatter(lr_xTe, lr_yTe, label=\"Test Points\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7976a17ca6f98815e5fe983f12f422e1",
     "grade": false,
     "grade_id": "cell-c9f425965ba3cfab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the following assignment there are a bunch of PyTorch specfic functions that we believe will be very helpful for you. Those are:\n",
    "\n",
    "* <code>torch.clamp(input, min=None, max=None, *, out=None) </code>: Clamps all elements in input into the range [min, max]\n",
    "\n",
    "* <code>torch.sum(input, *, dtype=None) </code>: Returns the sum of all elements in the input tensor.\n",
    "\n",
    "* <code>torch.mean(input, *, dtype=None)</code>: Returns the mean value of all elements in the input tensor.\n",
    "\n",
    "* <code>torch.pow(input, exponent, *, out=None)</code>: Takes the power of each element in input with exponent and returns a tensor with the result.\n",
    "\n",
    "* <code>torch.exp(input, *, out=None)</code>: Returns a new tensor with the exponential of the elements of the input tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac6cfc8ba2275fc304cfa5996429ac29",
     "grade": false,
     "grade_id": "cell-cdefaa85e4682b67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Linear classification\n",
    "\n",
    "<p> The first part of the assignment is to implement a linear support vector machine. In order to do this, we are going to generate random data to classify:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62333dc2402c1e11febda129998577e5",
     "grade": false,
     "grade_id": "cell-b99091c95c58a615",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def genrandomdata(n=100,b=0.):\n",
    "    # generate random data and linearly separable labels\n",
    "    xTr = np.random.randn(n, 2)\n",
    "    # defining random hyperplane\n",
    "    w0 = np.random.rand(2, 1)\n",
    "    # assigning labels +1, -1 labels depending on what side of the plane they lie on\n",
    "    yTr = np.sign(np.dot(xTr, w0)+b).flatten()\n",
    "    return torch.from_numpy(xTr).float(), torch.from_numpy(yTr).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2723cf8c87fcaa2f5afec7180393396c",
     "grade": false,
     "grade_id": "cell-f8347f448bc9596b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>Remember the SVM primal formulation\n",
    "$$\\begin{aligned}\n",
    "             &\\min_{\\mathbf{w},b,\\xi} \\|\\mathbf{w}\\|^2_2+C \\sum_{i=1}^n \\xi_i\\\\\n",
    "       & \\text{such that }  \\ \\forall i:\\\\\n",
    "             & y_i(\\mathbf{w}^\\top \\mathbf{x}_i+b)\\geq 1-\\xi_i\\\\\n",
    "             & \\xi_i\\geq 0.\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "You will need to implement  the function <code>primalSVM</code>, which takes in training data <code>xTr</code> ($n\\times d$) and labels <code>yTr</code> ($n$) with <code>yTr[i]</code>$\\in \\{-1,1\\}$. Note that we aren't doing linear programming, this is gradient descent optimization so the constraints are something we do not worry about.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a50a8718afa1e61def9c95628d23b3f9",
     "grade": false,
     "grade_id": "cell-bdf6fc745851b343",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To warm up, implement <code>hinge_loss</code>, which calculates the loss described in $\\sum_{i=1}^n \\xi_i$. Working with torch tensors is a lot like working with numpy tensors, think about the best way to do tensor on tensor operations. <b>This method requires no loops</b>.\n",
    "\n",
    "Hint: <code>torch.clamp</code> might be useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00794893f936e464629dd72ab98a53a2",
     "grade": false,
     "grade_id": "cell-eb14e96379dad7ed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def hinge_loss(y_pred, y_true):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef84adb5897d8abb4544789834245d13",
     "grade": false,
     "grade_id": "cell-d53bd9cf6a97a750",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, implement <code>LinearSVM</code>. This is a module (similar to the one in the example above) which initializes a linear classifer in dimension <code>dim</code>. In this module, you will need to initialize the necessary parameters for a linear model and define the forward pass for an input x. Hint: It <b>should</b> look very similar to what you have done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7ccf753a1f1ce2b3dfe23d90c9e1fef",
     "grade": false,
     "grade_id": "cell-767cf779d2d4b68e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Support Vector Machine\"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e004ffa096f6fcbe242e8ab305cd9ebf",
     "grade": false,
     "grade_id": "cell-bd3f1e535d4f9594",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, implement <code>primalSVM</code>. This is a method which takes in a set of training data <code>xTr</code> and labels <code>yTr</code>, a number of epochs <code>num_epochs</code> to train for, and our SVM <code>C</code> hyper-parameter. You should return a lambda function (https://www.w3schools.com/python/python_lambda.asp) <code>svmclassify</code> that produces a forward pass of your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b113ba1e695ab614a59d0de593fe224b",
     "grade": false,
     "grade_id": "cell-a1cbe4d52e9d7c21",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def primalSVM(xTr, yTr, num_epochs=10, C=1):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return svmclassify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f6a08a5830307ee949585f33435287e",
     "grade": false,
     "grade_id": "cell-54f91155ad52bf49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can test your SVM primal solver with the following randomly generated data set. We label it in a way that it is guaranteed to be linearly separable. If your code works correctly the hyper-plane should separate all the $x$'s into the red half and all the $o$'s into the blue half. With sufficiently large values of $C$ (e.g. $C>10$) you should obtain $0\\%$ training error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "942f15027724330b08b344e668853a21",
     "grade": false,
     "grade_id": "cell-5178de8375ab9209",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xTr,yTr = genrandomdata()\n",
    "fun = primalSVM(xTr,yTr,C=10)\n",
    "visclassifier.visclassifier(fun,xTr,yTr)\n",
    "err=torch.mean((torch.sign(fun(xTr))!=yTr).float())\n",
    "print(\"Training error: %2.1f%%\" % (err*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun(xTr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11abf96dc4f7de3ebf35cbb6a4c3098f",
     "grade": true,
     "grade_id": "cell-87d9677909636fa7",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 1: testCase_Primal\n",
    "# ------------------------------\n",
    "# Given a fixed training set, this tests if the signs of predictions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdfc1e20da5fe9690bb2f080f49fe20b",
     "grade": true,
     "grade_id": "cell-64ae8dc140286cbb",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 2: testCase_Primal\n",
    "# ------------------------------\n",
    "# Given a fixed training set, this tests if points farther from the decision boundary have larger predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f85ff9f9468f6188a1868a9ed424c9f1",
     "grade": false,
     "grade_id": "cell-fc6595df8ebc2f14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Spiral data set</h3>\n",
    "\n",
    "<p>The linear classifier works great in simple linear cases. But what if the data is more complicated? We provide you with a \"spiral\" data set. You can load it and visualize it with the following two code snippets:\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa6c57ab812d72e68303666a968b86f9",
     "grade": false,
     "grade_id": "cell-b707721c5c88ff46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N)\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    xTr = torch.tensor(xTr).float()\n",
    "    yTr = torch.tensor(yTr).float()\n",
    "    xTe = torch.tensor(xTe).float()\n",
    "    yTe = torch.tensor(yTe).float()\n",
    "    \n",
    "    vals, indices = torch.max(xTr, dim=0, keepdim=True)\n",
    "    xTr /= (vals * 2.0)\n",
    "    vals, indices = torch.max(xTe, dim=0, keepdim=True)\n",
    "    xTe /= (vals * 2.0)\n",
    "    \n",
    "    return xTr,yTr,xTe,yTe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09178ec0a696bdf2f2b3266681ea839e",
     "grade": false,
     "grade_id": "cell-09fd0d13d248f578",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "plt.figure()\n",
    "plt.scatter(xTr[yTr == 1, 0], xTr[yTr == 1, 1], c='b')\n",
    "plt.scatter(xTr[yTr != 1, 0], xTr[yTr != 1, 1], c='r')\n",
    "plt.legend([\"+1\",\"-1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79d94908d0146d80d4bc0275819987ef",
     "grade": false,
     "grade_id": "cell-0868475fb5c266a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>If you apply your previously functioning linear classifier on this data set you will see that you get terrible results. Your training error will increase drastically. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5994ef86bb9ea4809d2e782c2634fc83",
     "grade": false,
     "grade_id": "cell-7d1ebe38c2b8d1c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fun=primalSVM(xTr,yTr,C=10)\n",
    "visclassifier.visclassifier(fun,xTr,yTr)\n",
    "err=torch.mean(((torch.sign(fun(xTr)))!=yTr).float())\n",
    "print(\"Training error: %2.1f%%\" % (err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da87c8d08c600a434926b50ca3c8e50b",
     "grade": false,
     "grade_id": "cell-c7f79f4f5d5e55fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Implementing a kernelized SVM\n",
    "\n",
    "<p> For a data set as complex as the spiral data set, you will need a more complex classifier. \n",
    "First implement the kernel function\n",
    "<pre>\tcomputeK(kernel_type,X,Z,kpar)</pre>\n",
    "It takes as input a kernel type <code>kernel_type</code> and two data sets $\\mathbf{X} \\in \\mathcal{R}^{n\\times d}$ and $\\mathbf{Z} \\in \\mathcal{R}^{m\\times d}$ and outputs a kernel matrix $\\mathbf{K}\\in{\\mathcal{R}^{n\\times m}}$. The last input, <code>kpar</code> specifies the kernel parameter (e.g. the inverse kernel width $\\gamma$ in the RBF case or the degree $p$ in the polynomial case.)\n",
    "\t<ol>\n",
    "\t<li>For the linear kernel (<code>kernel_type='linear'</code>) svm, use $k(\\mathbf{x},\\mathbf{z})=x^Tz$ </li> \n",
    "\t<li>For the radial basis function kernel (<code>kernel_type='rbf'</code>) svm use $k(\\mathbf{x},\\mathbf{z})=\\exp(-\\gamma ||x-z||^2)$ (gamma is a hyperparameter, passed as the value of kpar)</li>\n",
    "\t<li>For the polynomial kernel (<code>kernel_type='poly'</code>) use  $k(\\mathbf{x},\\mathbf{z})=(x^Tz + 1)^d$ (d is the degree of the polymial, passed as the value of kpar)</li>\n",
    "</ol>\n",
    "\n",
    "<p>You can use the function <b><code>l2distance</code></b> as a helperfunction, which is located in defined in one of your starter files l2distance.py.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffaffe8f8033a6bd84dee60ea7bb1838",
     "grade": false,
     "grade_id": "cell-948d761b9f7671fe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def computeK(kernel_type, X, Z, kpar=0):\n",
    "    \"\"\"\n",
    "    function K = computeK(kernel_type, X, Z)\n",
    "    computes a matrix K such that Kij=k(x,z);\n",
    "    for three different function linear, rbf or polynomial.\n",
    "    \n",
    "    Input:\n",
    "    kernel_type: either 'linear','polynomial','rbf'\n",
    "    X: n input vectors of dimension d (nxd);\n",
    "    Z: m input vectors of dimension d (mxd);\n",
    "    kpar: kernel parameter (inverse kernel width gamma in case of RBF, degree in case of polynomial)\n",
    "    \n",
    "    OUTPUT:\n",
    "    K : nxm kernel Torch float tensor\n",
    "    \"\"\"\n",
    "    assert kernel_type in [\"linear\",\"polynomial\",\"poly\",\"rbf\"], \"Kernel type %s not known.\" % kernel_type\n",
    "    assert X.shape[1] == Z.shape[1], \"Input dimensions do not match\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ec6cb1bc06d8c89472292cb2688d170",
     "grade": true,
     "grade_id": "cell-caecc234e3612655",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 3: testCase_computeK_linear\n",
    "# ---------------------------------------\n",
    "# This tests whether the linear kernel is computed properly on an example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4208a4200946887dd4f4b7a9f1d6b362",
     "grade": true,
     "grade_id": "cell-15181fab6eaee5a1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 4: testCase_computeK_polynomial\n",
    "# -------------------------------------------\n",
    "# This tests whether the polynomial kernel is computed properly on an example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f2d3f1abdf7cd2d818877935037f009",
     "grade": true,
     "grade_id": "cell-8cd4b6888cf1b058",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 5: testCase_computeK_rbf\n",
    "# ------------------------------------\n",
    "# This tests whether the rbf kernel is computed properly on an example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ccf7ce40c01576ddcbbb800c93ff777",
     "grade": false,
     "grade_id": "cell-c1ebb4aee30b7b58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Additional Testing</h3>\n",
    "<p>The following code snippet plots an image of the kernel matrix for the data points in the spiral set. Use it to test your <b><code>computeK</code></b> function:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "K=computeK(\"rbf\",xTr,xTr,kpar=0.05)\n",
    "# plot an image of the kernel matrix\n",
    "plt.figure()\n",
    "plt.pcolormesh(K, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c7d6eb015dbf9b004eb3b34daf92141",
     "grade": false,
     "grade_id": "cell-6b72343891248ae1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Remember that the SVM optimization has the following dual formulation: (1)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "             &\\min_{\\alpha_1,\\cdots,\\alpha_n}\\frac{1}{2} \\sum_{i,j}\\alpha_i \\alpha_j y_i y_j \\mathbf{K}_{ij} - \\sum_{i=1}^{n}\\alpha_i  \\\\\n",
    "       \\text{s.t.}  &\\quad 0 \\leq \\alpha_i \\leq C\\\\\n",
    "             &\\quad \\sum_{i=1}^{n} \\alpha_i y_i = 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "This is equivalent to solving for the SVM primal (2)\n",
    "$$ L(\\mathbf{w},b) = C\\sum_{i=1}^n \\max(1-y_i(\\mathbf{w}^\\top\\phi(\\mathbf{x}_i)+b),0) + ||w||_2^2$$\n",
    "where $\\mathbf{w}=\\sum_{i=1}^n y_i \\alpha_i \\phi(\\mathbf{x}_i)$ and $\\mathbf{K}_{ij}=k(\\mathbf{x}_i,\\mathbf{x}_j)=\\phi(\\mathbf{x}_i)^\\top\\phi(\\mathbf{x}_j)$, for some mapping $\\phi(\\cdot)$. However, after a change of variable, with $\\beta_i = \\alpha_iy_i$ and $\\beta \\in R^n$, (2) can be rewritten as follows (see https://arxiv.org/pdf/1404.1066.pdf for details):\n",
    "$$ min_{\\beta, b} \\frac{1}{2}\\beta^\\top K\\beta + \\frac{C}{2}\\sum_{i=1}^n {[\\max(1-y_i(\\beta^\\top k_i+b),0)]}^2$$\n",
    "where $k_i$ is the kernel matrix row corresponding to the ith training example. Notice that there are two relaxations: 1. the $\\beta_i$ are unconstrained, in contrast to $\\alpha_i$ in (1), which must satisfy $0 \\leq \\alpha_i \\leq C$; and 2. the squared hinge loss is used in place of the more common absolute hinge loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b88370e20b4250f895bd19b2be93614f",
     "grade": false,
     "grade_id": "cell-478923c9beb252f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>\n",
    "    Implement the module \n",
    "    <pre>\n",
    "    KernelizedSVM(dim, kernel_type, kpar=0)\n",
    "    </pre>\n",
    "    This is a kernelized version of the SVM as defined above, which must maintain some kind of internal parameters for beta and b (hint: think what <code>dim</code> should be as a function of our training data) should be used for. Further, you are given <code>kernel_type</code> and <code>kpar</code>, which you should use in the creation of kernels by means of the method you wrote above <code>computeK</code>. For the forward pass of the kernelized SVM, recall that it is defined as $h(x) = w^\\top \\phi(x) + b$, where $w = \\sum_{i=1}^n \\beta_i\\phi(x_i)$. The output of your forward pass should be the classification itself of input data x.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "134b4b10c2005676c4399c0717a8a636",
     "grade": false,
     "grade_id": "cell-a6fae38b01273eed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class KernelizedSVM(nn.Module):\n",
    "    def __init__(self, dim, kernel_type, kpar=0):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def forward(self, xTr, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46f24d0e7b48723bbc00465e9cd35151",
     "grade": false,
     "grade_id": "cell-921592ddd34ba0d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>\n",
    "    Implement the function \n",
    "    <pre>\n",
    "    kernelsvm_loss(kernelizedSVM, kernel_mat, yTr, C)\n",
    "    </pre>\n",
    "    It should implement the loss function described above for the equivalent primal formulation of the dual:\n",
    "    $$ min_{\\beta, b} \\frac{1}{2}\\beta^\\top K\\beta + \\frac{C}{2}\\sum_{i=1}^n {[\\max(1-y_i(\\beta^\\top k_i+b),0)]}^2$$\n",
    "  You are given a KernalizedSVM module (<code>kernelizedSVM</code>) which you defined above, the kernel (<code>kernel_mat</code>), the training labels (<code>yTr</code>), and the regularizatin paramater (<code>C</code>). \n",
    " \n",
    "Note that this function <b>requires no loops</b>, and that you may find two functions especially helpful \n",
    "* <code>F.relu(x)</code> computes the <code>max(x,0)</code> in a way that allows for our optimizers to work (F is torch.nn.Functional, a library imported above) \n",
    "* <code>torch.square(x)</code> Returns a new tensor with the square of the elements of input.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21ffe0f7dfbee4627ab16cdc42aba416",
     "grade": false,
     "grade_id": "cell-f41db9a4afd212a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def kernelsvm_loss(kernelizedSVM, kernel_mat, yTr, C):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return cumulative_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e13472fb3026aeff310c689d9023882",
     "grade": false,
     "grade_id": "cell-640ffe0d367df416",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>\n",
    "    Implement the function \n",
    "    <pre>\n",
    "    dualSVM(xTr, yTr, kernel_type, num_epochs, C, kpar)\n",
    "    </pre>\n",
    "    It should use your functions <code><b>kernelsvm_loss</b></code>, <code><b>computeK</b></code>, and <code><b>KernelizedSVM</b></code> to solve the SVM dual problem of an SVM specified by a training data set (<code><b>xTr,yTr</b></code>), a regularization parameter (<code>C</code>), a kernel type (<code>ktype</code>) and kernel parameter (<code>kpar</code>), to be used as kpar in Kernel construction. This will once again be a training loop similar to the primalSVM above. You should return a lambda function <code>svmclassify</code> that produces a forward pass of your trained model.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25a58468eead0671575c4e8de49fa715",
     "grade": false,
     "grade_id": "cell-5da5e62eefa74972",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dualSVM(xTr, yTr, kernel_type, num_epochs=50, C=1, kpar=0):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return svmclassify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1695b107fe556c97f22f6285113c5867",
     "grade": false,
     "grade_id": "cell-4613065c19e1631f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Testing</h3>\n",
    "<p>Now we try the SVM with RBF kernel on the spiral data. If you implemented it correctly, train and test error should be close to zero.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "\n",
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "\n",
    "# poly kernel parameters that don't blow up vvv\n",
    "# ktype=\"poly\"\n",
    "# svmclassify=dualSVM(xTr, yTr, kernel_type=ktype, num_epochs=50, C=5.0, kpar=20)\n",
    "# visclassifier.visclassifier(svmclassify,xTr,yTr)\n",
    "\n",
    "# linear kernel parameters that also don't blow up vvv\n",
    "# ktype=\"linear\"\n",
    "# svmclassify=dualSVM(xTr, yTr, kernel_type=ktype, num_epochs=10, C=0.1)\n",
    "# visclassifier.visclassifier(svmclassify,xTr,yTr)\n",
    "\n",
    "# # rbf kernel with parameters that achieve perfect accuracy vvv \n",
    "ktype=\"rbf\"\n",
    "svmclassify=dualSVM(xTr, yTr, kernel_type=ktype, num_epochs=20, C=1, kpar=100)\n",
    "visclassifier.visclassifier(svmclassify,xTr,yTr)\n",
    "\n",
    "# compute training and testing error\n",
    "predsTr=svmclassify(xTr)\n",
    "trainingerr=torch.mean((torch.sign(predsTr)!=yTr).float())\n",
    "print(\"Training error: %2.4f\" % trainingerr)\n",
    "\n",
    "predsTe=svmclassify(xTe)\n",
    "testingerr=torch.mean((torch.sign(predsTe)!=yTe).float())\n",
    "print(\"Testing error: %2.4f\" % testingerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0aa2df4d10f8db8e37cedd2cba48bd4",
     "grade": false,
     "grade_id": "cell-ca3b1b5cd617938c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Testing Hint</h3> Create a dataset where you know what some of the optimal values of $\\alpha$ will be, and test to make sure that the solution gets those values of $\\alpha$ correct (recall from the lecture that the $\\alpha$ values associated with certain data points are guaranteed to have a specific optimal value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63dde3631e6115c1d1f8686a56e28978",
     "grade": true,
     "grade_id": "cell-bd5d833fe168a7a5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 6: testCase_dualSVM_easy_dataset\n",
    "# --------------------------------------------\n",
    "# This tests whether the function from dualSVM correctly classifies an example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba6613d659856839ad586e256ae539ec",
     "grade": true,
     "grade_id": "cell-567ead8163bdad82",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 7: testCase_dualSVM_hard_dataset\n",
    "# --------------------------------------------\n",
    "# This tests whether the function from dualSVM correctly classifies a hard example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ab95808c1c72a42df602edc932ab9ad",
     "grade": true,
     "grade_id": "cell-d6289896907469bc",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 8: testCase_dualSVM_hard_dataset2\n",
    "# ---------------------------------------------\n",
    "# This tests whether the function from dualSVM correctly classifies an even harder example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9798a21af6d0dd52df251c11e22d6e8e",
     "grade": false,
     "grade_id": "cell-d482c8b79d383b69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "SVMs are pretty sensitive to hyper-parameters. We ask you to implement a cross-validation function. <code>cross_validation</code> which takes training data <code>xTr</code>, training labels <code>yTr</code>, validation data <code>xValid</code>, validation labels <code>yValid</code>, kernel type <code>ktype</code>, list of possible C values <code>CList</code>, list of kernal parameter values for kernel generation <code>kparList</code>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1c4381b88a0334cae6926ab911b735f",
     "grade": false,
     "grade_id": "cell-523f3c6481d46ac1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(xTr,yTr,xValid,yValid,ktype,CList,kparList):\n",
    "    \"\"\"\n",
    "    function best_C,best_kpar,error_matrix = cross_validation(xTr,yTr,xValid,yValid,ktype,CList,kparList);\n",
    "    Use the parameter search to find the optimal parameter,\n",
    "    Individual models are trained on (xTr,yTr) while validated on (xValid,yValid)\n",
    "    \n",
    "    Input:\n",
    "        xTr      | training data (nxd)\n",
    "        yTr      | training labels (nx1)\n",
    "        xValid   | training data (mxd)\n",
    "        yValid   | training labels (mx1)\n",
    "        ktype    | the type of kernelization: 'rbf','polynomial','linear'\n",
    "        CList    | The list of values to try for the SVM regularization parameter C (ax1)\n",
    "        kparList | The list of values to try for the kernel parameter kpar- degree for poly, inverse width for rbf (bx1)\n",
    "    \n",
    "    Output:\n",
    "        best_C       | the best C parameter\n",
    "        best_kpar    | the best kpar parameter\n",
    "        error_matrix | the test error rate for each given (C, kpar) tuple when trained on (xTr,yTr) and tested on (xValid,yValid)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return best_C,best_kpar,error_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29067c8be390014b6702de8684179a40",
     "grade": false,
     "grade_id": "cell-48b1ca185eae90d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr,yTr,xValid,yValid=spiraldata(100)\n",
    "CList=(2.0**np.linspace(-1,5,7))\n",
    "kparList=(np.linspace(0.1,0.5,5))\n",
    "\n",
    "best_C,best_kpar,error_matrix = cross_validation(xTr,yTr,xValid,yValid,'rbf',CList,kparList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd2f48dc96555450ceedd74beb8c0cbf",
     "grade": true,
     "grade_id": "cell-80f8696818cafe2c",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 9: testCase_cv\n",
    "# ---------------------------\n",
    "# This tests whether the best hyperparameters found by cross validation are correct for an example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5971bcf5be5bd92cc5e24e13027b1b6c",
     "grade": false,
     "grade_id": "cell-8d9a00ede954a98e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Competition</h3>\n",
    "\n",
    "\n",
    "We ask you to implement function autosvm, which given xTr and yTr, splits them into training data and validation data, and then uses a hyperparameter search to find the optimal hyper parameters. \n",
    "\n",
    "Function autosvm should return a function which will act as a classifier on xTe.\n",
    "\n",
    "You have a 5 minute time limit on multiple datasets, each dataset having different optimal hyperparameters, so you should strive for a good method of finding hyperparameters (within the time limit) instead of just trying to find a static set of good hyperparameters. \n",
    "\n",
    "You will get full credit for the competition if you can beat the base benchmark of <b>46% error</b> (you will get partial credit if you beat <b>50% error</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbcd2378e6c43c6808d605b350e74351",
     "grade": false,
     "grade_id": "cell-2fbc5dbfb2b45cd3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def autosvm(xTr,yTr):\n",
    "    \"\"\"\n",
    "    svmclassify = autosvm(xTr,yTr), where yTe = svmclassify(xTe)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e00819254810d5fe2b00f4f7ed31f528",
     "grade": true,
     "grade_id": "cell-9db3d9cbb0582401",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 11: competition\n",
    "# ---------------------------\n",
    "# This tests the error rate of your classifier on the competition datasets \n",
    "# (remember each cell in this notebook should run in < 5 minutes!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiral_xTr,spiral_yTr,spiral_xTe,spiral_yTe=load_competitionBatch('spiral')\n",
    "spiral_yTr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XOR_xTr,XOR_yTr,XOR_xTe,XOR_yTe=load_competitionBatch('XOR')\n",
    "XOR_xTr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8561cb24065c7c8ef55483b5a06381c",
     "grade": true,
     "grade_id": "cell-4626183656c14d2e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Prints out the summary of tests passed/failed."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
